# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rul9B_KqlWUSDqdN1dULjejDb8hOy4Qj
"""

from google.colab import files
uploaded = files.upload()

!pip install -q lightgbm tqdm

import os
import re
import math
import gc
import requests
from io import BytesIO
from tqdm.auto import tqdm

import numpy as np
import pandas as pd

from sklearn.model_selection import KFold
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_absolute_error
from sklearn.impute import SimpleImputer

import lightgbm as lgb
from PIL import Image

TRAIN_CSV = "train.csv"
TEST_CSV  = "test.csv"
OUTPUT_CSV = "submission.csv"


USE_IMAGE_FEATURES = True


SEED = 42

def smape(y_true, y_pred):

    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    mask = denom == 0
    denom[mask] = 1
    res = np.abs(y_pred - y_true) / denom
    res[mask] = 0.0
    return 100.0 * np.mean(res)

print("Loading data...")
train = pd.read_csv(TRAIN_CSV)
test  = pd.read_csv(TEST_CSV)

print("Train shape:", train.shape, "Test shape:", test.shape)

def extract_ipq(text):
    if not isinstance(text, str):
        return np.nan

    patterns = [
        r'(\d+)\s*(?:pack|packs|pcs|pieces|pk|count)\b',
        r'pack of\s*(\d+)\b',
        r'(\d+)\s*[xX]\s*\d+',
        r'(\d+)[ ]?pcs\b',
        r'\bIPQ[:\s]*?(\d+)\b',
    ]
    for p in patterns:
        m = re.search(p, text, flags=re.IGNORECASE)
        if m:
            try:
                return float(m.group(1))
            except:
                pass
    m = re.search(r'\b(\d{1,2})\b', text)
    if m:
        return float(m.group(1))
    return np.nan

def basic_text_stats(text):
    if not isinstance(text, str):
        return {"chars":0, "words":0, "upper_ratio":0.0}
    chars = len(text)
    words = len(text.split())
    uppers = sum(1 for c in text if c.isupper())
    upper_ratio = uppers / max(chars, 1)
    return {"chars": chars, "words": words, "upper_ratio": upper_ratio}

def build_features(df, is_train=True):
    X = pd.DataFrame(index=df.index)

    df['catalog_content'] = df['catalog_content'].fillna('').astype(str)
    # IPQ
    X['ipq'] = df['catalog_content'].apply(extract_ipq)
    # Text stats
    txt_stats = df['catalog_content'].apply(basic_text_stats)
    X['chars'] = [t['chars'] for t in txt_stats]
    X['words'] = [t['words'] for t in txt_stats]
    X['upper_ratio'] = [t['upper_ratio'] for t in txt_stats]


    def extract_brand(text):
        if not isinstance(text, str) or len(text.strip())==0:
            return "unk"
        # try up to first 3 tokens (common brands)
        head = text.split(' - ')[0].split('-')[0].strip()
        head = head.split('â€¢')[0].strip()
        tokens = re.split(r'[\s,/|:;]+', head)
        if len(tokens) > 0:
            return tokens[0].lower()
        return "unk"
    df['brand'] = df['catalog_content'].apply(extract_brand)
    X['brand'] = df['brand']

    X['brand_hash'] = df['brand'].apply(lambda s: abs(hash(s)) % (10**6))
    return X

print("Building base features...")
X_train = build_features(train, is_train=True)
X_test  = build_features(test, is_train=False)

N_TEXT_COMPONENTS = 64
print("Vectorizing text into {} components...".format(N_TEXT_COMPONENTS))
hv = HashingVectorizer(n_features=2**16, ngram_range=(1,2), alternate_sign=False, norm='l2')
svd = TruncatedSVD(n_components=N_TEXT_COMPONENTS, random_state=SEED)

train_text = train['catalog_content'].fillna('').astype(str)
test_text  = test['catalog_content'].fillna('').astype(str)

train_h = hv.transform(train_text)
test_h = hv.transform(test_text)

print("Fitting SVD on training text matrix...")
svd.fit(train_h)
train_svd = svd.transform(train_h)
test_svd  = svd.transform(test_h)

for i in range(N_TEXT_COMPONENTS):
    X_train[f'text_svd_{i}'] = train_svd[:, i]
    X_test[f'text_svd_{i}']  = test_svd[:, i]

def extract_image_features(url, resize=(64,64)):
    try:
        resp = requests.get(url, timeout=5)
        img = Image.open(BytesIO(resp.content)).convert('RGB')
        img.thumbnail(resize)
        arr = np.array(img).astype(np.float32) / 255.0
        mean_rgb = arr.mean(axis=(0,1))
        h, w = arr.shape[:2]
        area = h * w

        hist = []
        for ch in range(3):

            hist_ch, _ = np.histogram(arr[:,:,ch], bins=8, range=(0,1))
            hist.extend(hist_ch.tolist())
        return list(mean_rgb) + [area] + hist
    except Exception as e:

        return [np.nan]*28

if USE_IMAGE_FEATURES:
    print("Extracting image features (this will take time)...")

    img_cols = [f'img_mean_{c}' for c in ['r','g','b']] + ['img_area'] + [f'img_hist_{i}' for i in range(24)]

    train_img_feats = []
    for url in tqdm(train['image_link'].fillna('').astype(str).values, desc='train images'):
        train_img_feats.append(extract_image_features(url))
    test_img_feats = []
    for url in tqdm(test['image_link'].fillna('').astype(str).values, desc='test images'):
        test_img_feats.append(extract_image_features(url))
    train_img_feats = np.array(train_img_feats, dtype=np.float32)
    test_img_feats  = np.array(test_img_feats, dtype=np.float32)
    for i,col in enumerate(img_cols):
        X_train[col] = train_img_feats[:, i]
        X_test[col]  = test_img_feats[:, i]
else:
    print("Skipping image features (USE_IMAGE_FEATURES=False).")

# Fill / scale numeric features
NUMERIC_COLUMNS = [c for c in X_train.columns if c not in ['brand', 'brand_hash']]

imputer = SimpleImputer(strategy='median')
scaler  = StandardScaler()

X_train_num = imputer.fit_transform(X_train[NUMERIC_COLUMNS])
X_train_num = scaler.fit_transform(X_train_num)

X_test_num  = imputer.transform(X_test[NUMERIC_COLUMNS])
X_test_num  = scaler.transform(X_test_num)


feature_names = [f for f in NUMERIC_COLUMNS]
X_tr = pd.DataFrame(X_train_num, columns=feature_names, index=X_train.index)
X_te = pd.DataFrame(X_test_num, columns=feature_names, index=X_test.index)


if 'brand_hash' not in X_tr.columns and 'brand_hash' in X_train.columns:
    X_tr['brand_hash'] = X_train['brand_hash']
    X_te['brand_hash']  = X_test['brand_hash']

print("Final train features shape:", X_tr.shape, "test features shape:", X_te.shape)


y = train['price'].values.astype(np.float32)

USE_LOG_TARGET = True
if USE_LOG_TARGET:
    y_model = np.log1p(y)
else:
    y_model = y

NFOLDS = 5
kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)
oof_preds = np.zeros(len(train))
test_preds = np.zeros(len(test))

lgb_params = {
    'objective': 'regression',
    'metric': 'rmse',
    'verbosity': -1,
    'boosting_type': 'gbdt',
    'seed': SEED,
    'learning_rate': 0.05,
    'num_leaves': 31,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'lambda_l1': 0.0,
    'lambda_l2': 0.0
}

print("Starting CV training with LightGBM...")
for fold, (tr_idx, val_idx) in enumerate(kf.split(X_tr, y_model)):
    print(f"Fold {fold+1}/{NFOLDS}")
    X_trn, X_val = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]
    y_trn, y_val = y_model[tr_idx], y_model[val_idx]
    dtrain = lgb.Dataset(X_trn, label=y_trn)
    dval   = lgb.Dataset(X_val, label=y_val)
    bst = lgb.train(
        lgb_params,
        dtrain,
        num_boost_round=5000,
        valid_sets=[dtrain, dval],
        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False), lgb.log_evaluation(period=200)], # Added log_evaluation callback

    )
    # Predict
    val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)
    test_pred = bst.predict(X_te, num_iteration=bst.best_iteration)
    oof_preds[val_idx] = val_pred
    test_preds += test_pred / NFOLDS

    del dtrain, dval, bst, X_trn, X_val
    gc.collect()

if USE_LOG_TARGET:
    oof_prices = np.expm1(oof_preds)
    test_prices = np.expm1(test_preds)
else:
    oof_prices = oof_preds
    test_prices = test_preds

# Calculate SMAPE after determining oof_prices
train_smape = smape(y, oof_prices)

print(f"OOF SMAPE on training data: {train_smape:.4f}%")

# Show sample predictions
preview = pd.DataFrame({
    'sample_id': train['sample_id'],
    'actual_price': y,
    'predicted_price_oof': oof_prices
}).sample(n=10, random_state=SEED)
print("Preview sample predictions:")
print(preview)

submission = pd.DataFrame({
    'sample_id': test['sample_id'],
    'price': test_prices
})
submission = submission[['sample_id', 'price']]
submission.to_csv(OUTPUT_CSV, index=False)
print(f"Saved submission to {OUTPUT_CSV} (rows: {len(submission)})")

from google.colab import files
files.download("submission.csv")